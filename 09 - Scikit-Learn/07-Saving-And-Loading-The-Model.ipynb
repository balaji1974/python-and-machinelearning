{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243de879-c5b5-410f-91e7-ea9a9736758d",
   "metadata": {},
   "source": [
    "# Saving and loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d39d60-9819-4f85-b6da-58ed9d2bb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a88fd1-ec74-4b5c-bfd1-74296c30175f",
   "metadata": {},
   "source": [
    "### Setting up the same data as last section (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ceb4a6d-148b-433d-b85b-5682da3f74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true: np.array, \n",
    "                   y_preds: np.array) -> dict:\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_pred labels.\n",
    "\n",
    "    Returns several metrics in the form of a dictionary.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2), \n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 score: {f1:.2f}\")\n",
    "\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf043fe-4414-40fb-9375-f90066b51e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameter grid similar to rs_clf.best_params_\n",
    "param_grid = {\"n_estimators\": [200, 1000],\n",
    "              \"max_depth\": [30, 40, 50],\n",
    "              \"max_features\": [\"log2\"],\n",
    "              \"min_samples_split\": [2, 4, 6, 8],\n",
    "              \"min_samples_leaf\": [4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b64cad-efca-4d7b-8147-6c942123447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   2.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.5s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.2s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.4s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   1.3s\n",
      "[INFO] The total running time for running GridSearchCV was 101.64 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Read in the data\n",
    "heart_disease = pd.read_csv(\"./resources/heart-disease.csv\") # load in from local directory\n",
    "\n",
    "# Shuffle the data (1 implies 100 data is shuffled)\n",
    "heart_disease = heart_disease.sample(frac=1)\n",
    "\n",
    "# Split into X (features) & y (labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Training and test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Set n_jobs to -1 to use all available machine cores (if this produces errors, try n_jobs=1)\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf = GridSearchCV(estimator=clf,\n",
    "                      param_grid=param_grid,\n",
    "                      cv=5, # 5-fold cross-validation\n",
    "                      verbose=2) # print out progress\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf\n",
    "gs_clf.fit(X_train, y_train);\n",
    "\n",
    "# Find the running time\n",
    "end_time = time.time()\n",
    "\n",
    "# How long did it take? \n",
    "total_time = end_time - start_time\n",
    "print(f\"[INFO] The total running time for running GridSearchCV was {total_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b983004-e242-4f7b-91c1-f254fb9d76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 80.33%\n",
      "Precision: 0.84\n",
      "Recall: 0.79\n",
      "F1 score: 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8, 'precision': 0.84, 'recall': 0.79, 'f1': 0.81}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics = evaluate_preds(y_test, gs_y_preds)\n",
    "gs_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a546c4-ef4f-4802-becf-ccfc5003fa96",
   "metadata": {},
   "source": [
    "# Two ways to save and load machine learning models:\n",
    "\n",
    "1. With Python's pickle module\n",
    "2. With the joblib module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b498972-3193-466d-a193-f5331be3dfb2",
   "metadata": {},
   "source": [
    "## 1. Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871868a-5787-496a-9d80-c8931dab2359",
   "metadata": {},
   "source": [
    "One way to save a model is using Python's `pickle` [module](https://docs.python.org/3/library/pickle.html).\n",
    "\n",
    "We'll use pickle's dump() method and pass it our model, gs_clf, along with the open() function containing a string for the filename we want to save our model as, along with the \"wb\" string which stands for \"write binary\", which is the file type open() will write our model as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5887fa9-da1e-45f2-83c7-ad8d87fdfbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save an extisting model to file\n",
    "pickle.dump(gs_clf, open(\"gs_random_random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3099292-654c-4bde-a807-eae49e938ba1",
   "metadata": {},
   "source": [
    "Once it's saved, we can import it using pickle's `load()` function, passing it `open()` containing the filename as a string and \"rb\" standing for \"read binary\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d525e5a0-9031-429d-b2b9-db3efe46f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "loaded_pickle_model = pickle.load(open(\"gs_random_random_forest_model_1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1bbd7d-fe2a-4a1b-8e12-828ead0990ac",
   "metadata": {},
   "source": [
    "Once you've reimported your trained model using pickle, you can use it to make predictions as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "59742715-0269-4420-a999-f1af13aefe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 80.33%\n",
      "Precision: 0.84\n",
      "Recall: 0.79\n",
      "F1 score: 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8, 'precision': 0.84, 'recall': 0.79, 'f1': 0.81}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions\n",
    "pickle_y_preds = loaded_pickle_model.predict(X_test)\n",
    "gs_metrics=evaluate_preds(y_test, pickle_y_preds)\n",
    "gs_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f00bfe-e907-424f-b1f1-390184677309",
   "metadata": {},
   "source": [
    "## 2. Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b8bb5-e8ae-4e05-a566-96eda224f1b4",
   "metadata": {},
   "source": [
    "The other way to load and save models is with `joblib`. Which works relatively the same as `pickle`.\n",
    "\n",
    "To save a model, we can use joblib's `dump()` function, passing it the model `(gs_clf)` and the desired `filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec32bcad-5ca4-4b50-a357-acc31cca38b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs_random_forest_model_1.joblib']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save model to file\n",
    "dump(gs_clf, filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2f5fc-5e10-49d0-b408-62667e051258",
   "metadata": {},
   "source": [
    "Once you've saved a model using `dump()`, you can import it using `load()` and passing it the filename of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c55bf22b-1659-4cdf-9807-992d8c3f624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a saved joblib model\n",
    "loaded_joblib_model = load(filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d040943-b126-4ba5-973f-fe42b92a3353",
   "metadata": {},
   "source": [
    "Once imported, we can make predictions with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20496e6e-f140-4ae9-aee0-06cd70831de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 80.33%\n",
      "Precision: 0.84\n",
      "Recall: 0.79\n",
      "F1 score: 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8, 'precision': 0.84, 'recall': 0.79, 'f1': 0.81}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make and evaluate joblib predictions\n",
    "joblib_y_preds = loaded_joblib_model.predict(X_test)\n",
    "loaded_joblib_model_metrics = evaluate_preds(y_test, joblib_y_preds)\n",
    "loaded_joblib_model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de01f7-694e-464d-803a-c13654ee30b4",
   "metadata": {},
   "source": [
    "And once again, you'll notice the evaluation metrics are the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fd986711-bcdd-475c-90ed-6f647753ba8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_joblib_model_metrics == gs_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63512f8-6b8e-46ec-9b33-5541488879c8",
   "metadata": {},
   "source": [
    "So which one should you use, `pickle` or `joblib`?\n",
    "\n",
    "According to [Scikit-Learn's model persistence documentation](https://scikit-learn.org/stable/model_persistence.html), they suggest it may be more efficient to use `joblib` as it's more efficient with large numpy arrays (which is what may be contained in trained/fitted Scikit-Learn models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
